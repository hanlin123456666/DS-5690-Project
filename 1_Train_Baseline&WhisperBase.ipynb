{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "5a09b24d",
      "metadata": {
        "id": "5a09b24d"
      },
      "source": [
        "# Data Access and Transformation for Speech Classification\n",
        "\n",
        "Cleft palate dataset that is analyzed to identify hypernasality in speech"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de5f58f0",
      "metadata": {
        "id": "de5f58f0"
      },
      "source": [
        "## 1. Importing Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d294c2c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d294c2c",
        "outputId": "35acdf53-f650-4659-b7df-b658a01ba2d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.1.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.1.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "import datasets\n",
        "import pandas as pd\n",
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import WhisperModel, WhisperFeatureExtractor\n",
        "from datasets import load_from_disk\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beb23d48",
      "metadata": {
        "id": "beb23d48"
      },
      "source": [
        "## 2. Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "KmeBVIXE32bA"
      },
      "id": "KmeBVIXE32bA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oq1poblMcq6H",
        "outputId": "5cc21394-a4d4-4cfc-f273-c2aa6a37fc88"
      },
      "id": "Oq1poblMcq6H",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd #check where I am"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRfZgWZTu_ji",
        "outputId": "82b0065d-9b87-48ea-9798-f38d410d28d7"
      },
      "id": "zRfZgWZTu_ji",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd '/content/drive/MyDrive/vandy 24fall/Transformer/public_samples'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I0uNgYQu03h",
        "outputId": "9b84bed4-92b8-4e03-e6d5-8b6575f7bed2"
      },
      "id": "5I0uNgYQu03h",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/vandy 24fall/Transformer/public_samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls #check what do I have"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc0yTzvXy_Aw",
        "outputId": "1f1772bf-9af6-4773-8361-3c4369928766"
      },
      "id": "pc0yTzvXy_Aw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CASES\t     CONTROLS\t   test_dataset      train_dataset\tval_dataset.zip\n",
            "CASES_WAV    CONTROLS_WAV  test_dataset.zip  train_dataset.zip\twhisper_best_model.pt\n",
            "catalog.csv  test.csv\t   train.csv\t     val_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71102a34",
      "metadata": {
        "id": "71102a34"
      },
      "outputs": [],
      "source": [
        "train_audio_dataset = load_from_disk(\"train_dataset\")\n",
        "test_audio_dataset = load_from_disk(\"test_dataset\")\n",
        "val_audio_dataset = load_from_disk(f\"val_dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# explore the data\n",
        "# audio has 1)path: the path to the audio file on your disk 2) array: a numpy array representing the raw audio waveform, each value corresponds to a sampled amplitude of the audio signal 3) sample rate: sampling rate of the audio signal\n",
        "print(train_audio_dataset[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLktDOk-8X2F",
        "outputId": "8baae0ca-d06f-4019-a93e-dc3bc10af3b2"
      },
      "id": "vLktDOk-8X2F",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'audio': {'path': 'ACPA Santa came home since the snow fell.wav', 'array': array([-9.34825897e-11, -2.67201782e-11, -8.02570579e-11, ...,\n",
            "       -5.39624239e-07,  4.85038470e-07,  0.00000000e+00]), 'sampling_rate': 16000}, 'labels': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "L6tZqRGEQLRs"
      },
      "id": "L6tZqRGEQLRs"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch\n",
        "!pip install datasets\n",
        "!pip install librosa\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "Fd0-nd-GQ_9V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1244d823-3a62-4c83-8934-95bc9af95ef2"
      },
      "id": "Fd0-nd-GQ_9V",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.9)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.5.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.12.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2024.8.30)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import datasets\n",
        "from datasets import load_dataset, DatasetDict,  Audio, load_from_disk\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import librosa\n",
        "import io\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
        "from transformers import WhisperModel, WhisperFeatureExtractor, AdamW\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score"
      ],
      "metadata": {
        "id": "FKTqMBNXRDah"
      },
      "id": "FKTqMBNXRDah",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline\n",
        "- Support Vector Machine (SMV) and Random Forest (RF) model act as a baseline for the LLM training.\n",
        "- References: https://medium.com/@mujtabaraza194/voice-classification-using-mfcc-features-and-deep-neural-networks-a-step-by-step-guide-296670ae1e79"
      ],
      "metadata": {
        "id": "e-Dx3ZK3Qtik"
      },
      "id": "e-Dx3ZK3Qtik"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVM"
      ],
      "metadata": {
        "id": "vv56NRfE8uSL"
      },
      "id": "vv56NRfE8uSL"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "tsd_JZBf9WNe"
      },
      "id": "tsd_JZBf9WNe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "ei6h07f2mnoT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "778366dd-552b-4d7f-d467-59401aafaa87"
      },
      "id": "ei6h07f2mnoT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/vandy 24fall/Transformer/public_samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OE04SIe1rn30",
        "outputId": "5bbe87f3-0b2a-44a0-f3a8-d89cad56fcf3"
      },
      "id": "OE04SIe1rn30",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CASES\t     CONTROLS\t   test_dataset      train_dataset\tval_dataset.zip\n",
            "CASES_WAV    CONTROLS_WAV  test_dataset.zip  train_dataset.zip\twhisper_best_model.pt\n",
            "catalog.csv  test.csv\t   train.csv\t     val_dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")"
      ],
      "metadata": {
        "id": "s5mly9oGrMWf"
      },
      "id": "s5mly9oGrMWf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train set\n",
        "train_files = train_df[\"WAV_filename\"].tolist()\n",
        "\n",
        "train_folder = train_df[\"WAV_folder\"].tolist()\n",
        "\n",
        "train_full_paths = [os.path.join(train_folder[i], train_files[i]) for i in range(0,len(train_files))]\n",
        "\n",
        "train_labels = train_df[\"hypernasality\"].tolist()"
      ],
      "metadata": {
        "id": "8VDRJXEEq_44"
      },
      "id": "8VDRJXEEq_44",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test set\n",
        "test_files = test_df[\"WAV_filename\"].tolist()\n",
        "\n",
        "test_folder = test_df[\"WAV_folder\"].tolist()\n",
        "\n",
        "test_full_paths = [os.path.join(test_folder[i], test_files[i]) for i in range(0,len(test_files))]\n",
        "\n",
        "test_labels = test_df[\"hypernasality\"].tolist()"
      ],
      "metadata": {
        "id": "oYQ8pq67r6Zu"
      },
      "id": "oYQ8pq67r6Zu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to extract MFCCs from an audio file\n",
        "def extract_mfcc_features(file_path, n_mfcc=13):\n",
        "    # print(file_path)\n",
        "    audio, sample_rate = librosa.load(file_path, sr=None)\n",
        "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n",
        "    mfccs_scaled = np.mean(mfccs.T, axis=0)  # Taking the average across time\n",
        "    return mfccs_scaled\n",
        "\n",
        "# Paths to your audio files\n",
        "audio_files = train_full_paths + test_full_paths  # Add more paths as needed\n",
        "labels = train_labels + test_labels  # Corresponding labels for your audio files\n",
        "\n",
        "# Extract features from each audio file\n",
        "features = [extract_mfcc_features(file) for file in audio_files]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
        "x_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standardize features by removing the mean and scaling to unit variance\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(x_train)\n",
        "X_test = scaler.transform(x_test)\n",
        "\n",
        "# Initialize and train the SVM classifier\n",
        "svm_model = SVC(kernel='linear')  # You can experiment with different kernels\n",
        "svm_model.fit(x_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = svm_model.predict(x_val)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
        "print(\"Classification Report:\", classification_report(y_val, y_pred))"
      ],
      "metadata": {
        "id": "q-zCCFiN9aCl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1891c52b-51ab-481b-ef37-3ecf0561e61c"
      },
      "id": "q-zCCFiN9aCl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CASES_WAV/ACPA ted had a dog with white feet-3.wav\n",
            "CONTROLS_WAV/cdc 4 (and then go to school).wav\n",
            "CONTROLS_WAV/Video 1_4 (and can I have some more material).wav\n",
            "CONTROLS_WAV/NEW - video 2 (three times).wav\n",
            "CONTROLS_WAV/cdc 4 (and then he brushed his teeth).wav\n",
            "CONTROLS_WAV/NEW - video 2 (no they dont talk).wav\n",
            "CONTROLS_WAV/cdc 2 (I said thank you ray).wav\n",
            "CASES_WAV/Video 1_9 (pa, pa, pa).wav\n",
            "CONTROLS_WAV/ACPA We shouldn_t play in the street.wav\n",
            "CONTROLS_WAV/ACPA look at this book with us.wav\n",
            "CASES_WAV/Facebook  (pick up the pie).wav\n",
            "CASES_WAV/ACPA Tom had ham and eggs for breakfast.wav\n",
            "CASES_WAV/Video 6_7 (buy baby a bib).wav\n",
            "CONTROLS_WAV/video 1 (puppy are you ready_).wav\n",
            "CONTROLS_WAV/Video 1_18 (pretend it stops running when the car is going).wav\n",
            "CONTROLS_WAV/NEW - video 2 (bugs and spiders, I protect).wav\n",
            "CONTROLS_WAV/video 1 (yes I am going to give him coffee for me and him).wav\n",
            "CASES_WAV/Video 3_6 (muddy 2).wav\n",
            "CONTROLS_WAV/cdc 4 (and then he was a boy).wav\n",
            "CONTROLS_WAV/NEW - video 2 (like drums).wav\n",
            "CONTROLS_WAV/video 1 (shh).wav\n",
            "CASES_WAV/Facebook (pick up the baby).wav\n",
            "CASES_WAV/Video 3_5 (muddy).wav\n",
            "CASES_WAV/ACPA Tom had ham and eggs for breakfast-2.wav\n",
            "CASES_WAV/ACPA opening and closing boxes.wav\n",
            "CASES_WAV/Video 1_5 (see the seesaw).wav\n",
            "CASES_WAV/ACPA playing in the snow is fun-4.wav\n",
            "CASES_WAV/NEW - video 7 (puffy).wav\n",
            "CONTROLS_WAV/cdc 7 (yellow).wav\n",
            "CONTROLS_WAV/NEW - video 2 (play some games).wav\n",
            "CASES_WAV/ACPA whatever they need.wav\n",
            "CONTROLS_WAV/ACPA cookies are good to eat-2.wav\n",
            "CONTROLS_WAV/Video 1_9 (where_s my bowl).wav\n",
            "CASES_WAV/ACPA we shouldn_t play in the street-3.wav\n",
            "CONTROLS_WAV/cdc 7 (blue).wav\n",
            "CASES_WAV/ACPA transferring, getting information to customers.wav\n",
            "CONTROLS_WAV/Video 1_11 (who, who).wav\n",
            "CONTROLS_WAV/Video 1_17 (but if its not, then we would stop running).wav\n",
            "CONTROLS_WAV/ACPA Most Boys Like to Play Football.wav\n",
            "CASES_WAV/ACPA I like ice cream.wav\n",
            "CASES_WAV/ACPA in the safe deposit department.wav\n",
            "CONTROLS_WAV/Video 1_3 (its all of our birthdays).wav\n",
            "CONTROLS_WAV/NEW - video 2 (can I have your phone).wav\n",
            "CONTROLS_WAV/ACPA cookies are good to eat.wav\n",
            "CONTROLS_WAV/cdc 4 (and then he ate breakfast).wav\n",
            "CONTROLS_WAV/ACPA I like school-2.wav\n",
            "CONTROLS_WAV/Video 1_13 (yes I_m going to give him some coffee for me and him).wav\n",
            "CONTROLS_WAV/video 1 (puppies).wav\n",
            "CASES_WAV/Video 2_9 (she went shopping).wav\n",
            "CASES_WAV/ACPA Nick_s grandmother lives in the city-3.wav\n",
            "CASES_WAV/ACPA we went to town yesterday.wav\n",
            "CONTROLS_WAV/ACPA they feed seeds to birds-2.wav\n",
            "CONTROLS_WAV/Video 1_10 (and where_s the corn sauce).wav\n",
            "CASES_WAV/ACPA Nick_s grandmother lives in the city.wav\n",
            "CASES_WAV/Video 1_6 (sha, sha, sha).wav\n",
            "CASES_WAV/Video 6_4 (bah).wav\n",
            "CASES_WAV/Video 2_10 (I ride a choo-choo train).wav\n",
            "CASES_WAV/Facebook (susie sees the dress).wav\n",
            "CASES_WAV/ACPA ted had a dog with white feet-4.wav\n",
            "CASES_WAV/Video 3_7 (tie and hat to eat dinner).wav\n",
            "CASES_WAV/Video 6_3 (puppy will pull a rope).wav\n",
            "CASES_WAV/ACPA one is in school in Arizona, his father is going out there this weekend to check on him.wav\n",
            "CASES_WAV/Video 2_5 (baby).wav\n",
            "CASES_WAV/Facebook (susie sees the horse).wav\n",
            "CASES_WAV/Video 6_2 (Popeye).wav\n",
            "CONTROLS_WAV/ACPA Ted had a dog with white feet.wav\n",
            "CASES_WAV/Video 6_1 (mom).wav\n",
            "CONTROLS_WAV/Video 1_16 (its broke, I broke it).wav\n",
            "CASES_WAV/Facebook (go get the cookie).wav\n",
            "CONTROLS_WAV/ACPA sue roasted a duck for supper.wav\n",
            "CASES_WAV/Facebook (pick up the books).wav\n",
            "CASES_WAV/Video 4_4 (well it will help me).wav\n",
            "CONTROLS_WAV/cdc 1 (thank you ray).wav\n",
            "CASES_WAV/Video 1_4 (seesaw).wav\n",
            "CONTROLS_WAV/ACPA Playing in the snow is fun.wav\n",
            "CONTROLS_WAV/NEW - video 2 (but).wav\n",
            "CONTROLS_WAV/NEW - video 2 (but ants do that sometimes).wav\n",
            "CONTROLS_WAV/NEW - video 2 (say just hello for pretend).wav\n",
            "CONTROLS_WAV/ACPA Santa came home since the snow fell-2.wav\n",
            "CONTROLS_WAV/cdc 3 (we got him).wav\n",
            "CASES_WAV/Video 1_11 (3-9).wav\n",
            "CASES_WAV/ACPA playing in the snow is fun-3.wav\n",
            "CASES_WAV/Video 5_4 (smashing).wav\n",
            "CASES_WAV/Video 2_3 (do it for daddy).wav\n",
            "CONTROLS_WAV/cdc 8 (went to the park).wav\n",
            "CONTROLS_WAV/ACPA Do you have a brother or a sister.wav\n",
            "CASES_WAV/ACPA my other son is a sophomore in high school.wav\n",
            "CASES_WAV/ACPA playing in the snow is fun-5.wav\n",
            "CASES_WAV/ACPA most boys like to play football-4.wav\n",
            "CASES_WAV/Video 1_10 (bah, bah, bah).wav\n",
            "CONTROLS_WAV/cdc 3 (that_s a ladybug house, right_)-2.wav\n",
            "CASES_WAV/Video 1_1 (1-9).wav\n",
            "CASES_WAV/Video 2_4 (daddy).wav\n",
            "CONTROLS_WAV/ACPA Santa came home since the snow fell.wav\n",
            "CASES_WAV/Video 3_2 (ted and todd).wav\n",
            "CONTROLS_WAV/cdc 8 (I played with my toys).wav\n",
            "CONTROLS_WAV/cdc 7 (green).wav\n",
            "CASES_WAV/NEW - video 7 (puppy).wav\n",
            "CASES_WAV/NEW - video 7 (buy bobby a puppy).wav\n",
            "CASES_WAV/ACPA do you have a brother or a sister-2.wav\n",
            "CASES_WAV/Video 6_5 (bobby).wav\n",
            "CASES_WAV/ACPA do you have a brother or a sister-3.wav\n",
            "CASES_WAV/ACPA we shouldn_t play in the street-2.wav\n",
            "CONTROLS_WAV/NEW - video 2 (they just sit on the table and I say hi to them).wav\n",
            "CONTROLS_WAV/Video 1_5 (do you have a knife).wav\n",
            "CASES_WAV/Facebook (susie sees the scissors).wav\n",
            "CASES_WAV/ACPA buy baby a bib.wav\n",
            "CASES_WAV/Video 2_1 (put the baby in the buggy).wav\n",
            "CONTROLS_WAV/cdc 6 (the polar bears are dancing).wav\n",
            "CONTROLS_WAV/video 1 (stop).wav\n",
            "CONTROLS_WAV/Video 1_14 (fill up the gas).wav\n",
            "CONTROLS_WAV/video 1 (where_s the burner).wav\n",
            "CONTROLS_WAV/cdc 8( I swung on the swings and did other stuff).wav\n",
            "CASES_WAV/Facebook (take a turtle).wav\n",
            "CONTROLS_WAV/video 1 (fire).wav\n",
            "CONTROLS_WAV/NEW - video 2 (they don_t bite me either).wav\n",
            "CASES_WAV/Video 3_4 (don_t wear your dirty).wav\n",
            "CONTROLS_WAV/Video 1_6 (good).wav\n",
            "CONTROLS_WAV/video 1 (four).wav\n",
            "CASES_WAV/NEW - video 7 (sissy sees the sun).wav\n",
            "CONTROLS_WAV/cdc 7 (red).wav\n",
            "CONTROLS_WAV/NEW - video 2 (where is it).wav\n",
            "CASES_WAV/Facebook (take a teddybear).wav\n",
            "CONTROLS_WAV/cdc 3 (he won_t fly away).wav\n",
            "CONTROLS_WAV/NEW - video 2 (can I see_).wav\n",
            "CASES_WAV/ACPA and I started out on the switch board and two phones.wav\n",
            "CASES_WAV/ACPA we go swimming on a very hot day.wav\n",
            "CASES_WAV/Facebook (go get the car).wav\n",
            "CASES_WAV/ACPA most boys like to play football-6.wav\n",
            "CONTROLS_WAV/Video 1_2 (shh).wav\n",
            "CASES_WAV/Facebook (go get the cake).wav\n",
            "CONTROLS_WAV/ACPA I like school.wav\n",
            "CASES_WAV/NEW - video 7 (pamper).wav\n",
            "CASES_WAV/ACPA playing in the snow is fun-2.wav\n",
            "CASES_WAV/Video 4_3 (I struggle with my _s_).wav\n",
            "CASES_WAV/Video 1_7 (ca, ca, ca).wav\n",
            "CASES_WAV/Video 2_2 (take teddy to town).wav\n",
            "CASES_WAV/Video 4_5 (but it will help me with the _s_).wav\n",
            "CASES_WAV/Video 3_1 (told).wav\n",
            "CONTROLS_WAV/ACPA she wore blue shoes.wav\n",
            "CONTROLS_WAV/cdc 5 (can I play with Jack).wav\n",
            "CONTROLS_WAV/Video 1_15 (it stops going).wav\n",
            "CONTROLS_WAV/video 1 (pizza bundt).wav\n",
            "CASES_WAV/ACPA most boys like to play football-3.wav\n",
            "CASES_WAV/Facebook  (take a tire).wav\n",
            "CASES_WAV/Video 5_1 (feet).wav\n",
            "CONTROLS_WAV/Video 1_7 (here_s some pizza).wav\n",
            "CONTROLS_WAV/video 1 (door and four).wav\n",
            "CASES_WAV/Video 1_2 (shoe).wav\n",
            "CASES_WAV/Facebook (sixty, sixty, sixty).wav\n",
            "CONTROLS_WAV/Video 1_19 (tires).wav\n",
            "CONTROLS_WAV/NEW - video 2 (bye-bye_.wav\n",
            "CASES_WAV/ACPA we shouldn_t play in the street-5.wav\n",
            "CONTROLS_WAV/cdc 2 (where_d dad go).wav\n",
            "CONTROLS_WAV/video 1 (do you like milk in your tea_).wav\n",
            "CONTROLS_WAV/cdc 3 (don_t open it, its locked).wav\n",
            "CASES_WAV/Video 5_3 (I want a picture of giant man).wav\n",
            "CONTROLS_WAV/Video 1_8 (And I_ll put some).wav\n",
            "CASES_WAV/ACPA Jack likes cheese sandwiches for lunch.wav\n",
            "CONTROLS_WAV/video 1 (the pizza).wav\n",
            "CASES_WAV/Video 2_7 (go get the wagon).wav\n",
            "CONTROLS_WAV/NEW - video 2 (I say hello to spiders because they_re nice to me).wav\n",
            "CONTROLS_WAV/Video 1_12 (papi are you ready).wav\n",
            "CASES_WAV/ACPA we shouldn_t play in the street-4.wav\n",
            "CASES_WAV/Video 1_8 (ma, ma, ma).wav\n",
            "CONTROLS_WAV/video 1 (where_s the sauce_).wav\n",
            "CASES_WAV/Video 1_3 (shiny shoes).wav\n",
            "CASES_WAV/Video 2_8 (I see the sun in the sky).wav\n",
            "CASES_WAV/Video 2_6 (I eat cake and ice cream).wav\n",
            "CASES_WAV/ACPA Most boys like to play football-2.wav\n",
            "CASES_WAV/Video 5_2 (fence).wav\n",
            "CONTROLS_WAV/cdc 3 (yeah).wav\n",
            "CONTROLS_WAV/ACPA they feed seeds to birds.wav\n",
            "CONTROLS_WAV/Video 1_1 (we have to have it first, then the cake).wav\n",
            "CONTROLS_WAV/ACPA she wore blue shoes-2.wav\n",
            "CASES_WAV/Video 4_1 (it was really fun, I think it really helps me, it makes me feel great).wav\n",
            "CONTROLS_WAV/NEW - video 2 (you have pants on).wav\n",
            "CASES_WAV/ACPA most boys like to play football-5.wav\n",
            "CASES_WAV/Video 6_6 (bobby 2).wav\n",
            "CASES_WAV/ACPA 2 more in grammar school, and out little 3 year old.wav\n",
            "CONTROLS_WAV/ACPA look at this book with us-2.wav\n",
            "CASES_WAV/ACPA Ted had a dog with white feet-2.wav\n",
            "CONTROLS_WAV/video 1 (we dont have any pineapple).wav\n",
            "CASES_WAV/ACPA oh let me see, what can I tell you about them now.wav\n",
            "Accuracy: 0.8461538461538461\n",
            "Classification Report:               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.93      0.74      0.82        19\n",
            "         1.0       0.79      0.95      0.86        20\n",
            "\n",
            "    accuracy                           0.85        39\n",
            "   macro avg       0.86      0.84      0.84        39\n",
            "weighted avg       0.86      0.85      0.84        39\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on the test set\n",
        "test_predictions = svm_model.predict(X_test)\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, test_predictions))\n",
        "print(\"Test Classification Report:\", classification_report(y_test, test_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nFI1r2otsta",
        "outputId": "0558bde3-8486-43af-b312-e8e39a59e9a1"
      },
      "id": "4nFI1r2otsta",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.5892857142857143\n",
            "Test Classification Report:               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.59      1.00      0.74        33\n",
            "         1.0       0.00      0.00      0.00        23\n",
            "\n",
            "    accuracy                           0.59        56\n",
            "   macro avg       0.29      0.50      0.37        56\n",
            "weighted avg       0.35      0.59      0.44        56\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random forest"
      ],
      "metadata": {
        "id": "ikFivPReIi81"
      },
      "id": "ikFivPReIi81"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# Initialize and train the Random Forest classifier\n",
        "rf_model = RandomForestClassifier(n_estimators=100)  # You can adjust the number of trees\n",
        "rf_model.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions - VAL\n",
        "y_pred = rf_model.predict(x_val)\n",
        "\n",
        "# Evaluate the classifier\n",
        "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
        "print(\"Classification Report:\", classification_report(y_val, y_pred))"
      ],
      "metadata": {
        "id": "uj25Oq1Q6uOg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fe12ac6-7145-46da-9e8f-f50cc0ec9135"
      },
      "id": "uj25Oq1Q6uOg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9487179487179487\n",
            "Classification Report:               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      0.89      0.94        19\n",
            "         1.0       0.91      1.00      0.95        20\n",
            "\n",
            "    accuracy                           0.95        39\n",
            "   macro avg       0.95      0.95      0.95        39\n",
            "weighted avg       0.95      0.95      0.95        39\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "test_predictions = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "print(\"Test Accuracy:\", accuracy_score(y_test, test_predictions))\n",
        "print(\"Test Classification Report:\", classification_report(y_test, test_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSZ9NJ5tu6vS",
        "outputId": "7fb54e39-60c6-4fc8-e28e-0b7ff334dd2b"
      },
      "id": "CSZ9NJ5tu6vS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.6071428571428571\n",
            "Test Classification Report:               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.61      0.94      0.74        33\n",
            "         1.0       0.60      0.13      0.21        23\n",
            "\n",
            "    accuracy                           0.61        56\n",
            "   macro avg       0.60      0.53      0.48        56\n",
            "weighted avg       0.60      0.61      0.52        56\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Whisper\n"
      ],
      "metadata": {
        "id": "pWf441TVnjAb"
      },
      "id": "pWf441TVnjAb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Whisper-base"
      ],
      "metadata": {
        "id": "kwHAkLTInvnK"
      },
      "id": "kwHAkLTInvnK"
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"openai/whisper-base\"\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(model_checkpoint)\n",
        "encoder = WhisperModel.from_pretrained(model_checkpoint)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "d292282dc2964d448393ecb473c6f89c",
            "132629fe9e91496292f86cb40bc623a8",
            "8bd2e279d4014af5bfa859c7cb7d70c9",
            "4f7b7fbe3f4a48f88959a8d20a99c1e2",
            "1944748837404dada3fd8af9ac11d3fd",
            "e60a402ad7114725921609a879f330bd",
            "e96de1b2ca67498993dceaca81f64e29",
            "3e6c8edc6d5546e599f746670d89ef04",
            "0b3412f6b67146bcaef301019bc1aaeb",
            "f5817e5b97924be6ac45769e2723a35e",
            "26c607c3043f4f13ae6fb7d6dde86224",
            "53b889cc029d4412817039a59179d6d9",
            "7df6bb9be5fb4cf0bf71b101f4267e40",
            "340af6d629a7403692c19c800586ae19",
            "f693e41010b440b888d47a72725ca426",
            "c1164e4fe28d4220bd383b8b6b75a36d",
            "c77009d88eb349a08e257e491b98f3f7",
            "692db9dae5b5403cb12c64f9df367ba7",
            "40d581b818624a31a730b173e6b68d4f",
            "d83d89ce919843ea9cf93787aabec2ae",
            "4a0d6a6a41ac4073a4fe1c3c758f6757",
            "1130d4d9c17d4ca39e9cf851b8836158"
          ]
        },
        "id": "KQRTquDYnh8q",
        "outputId": "2bf676ee-9300-4b27-94e3-22a08eccf7f8"
      },
      "id": "KQRTquDYnh8q",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.98k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d292282dc2964d448393ecb473c6f89c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/290M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53b889cc029d4412817039a59179d6d9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train"
      ],
      "metadata": {
        "id": "-NO3q9BAodlD"
      },
      "id": "-NO3q9BAodlD"
    },
    {
      "cell_type": "code",
      "source": [
        "class SpeechClassifier(nn.Module):\n",
        "    def __init__(self, num_labels, encoder):\n",
        "        super(SpeechClassifier, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(self.encoder.config.hidden_size, 4096),\n",
        "            nn.ReLU (),\n",
        "            nn.Linear(4096, 2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2048, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, num_labels)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_features, decoder_input_ids):\n",
        "        outputs = self.encoder(input_features, decoder_input_ids=decoder_input_ids)\n",
        "        pooled_output = outputs['last_hidden_state'][:, 0, :]\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "num_labels = 2\n",
        "\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, betas=(0.9, 0.999), eps=1e-08)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# Define the training function\n",
        "def train(model, train_loader, val_loader, optimizer,  criterion, device, num_epochs):\n",
        "\n",
        "    best_accuracy = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for i, batch in enumerate(train_loader):\n",
        "\n",
        "            input_features, decoder_input_ids, labels = batch\n",
        "\n",
        "            input_features = input_features.squeeze()\n",
        "            input_features = input_features.to(device)\n",
        "\n",
        "            decoder_input_ids = decoder_input_ids.squeeze()\n",
        "            decoder_input_ids = decoder_input_ids.to(device)\n",
        "\n",
        "            labels = labels.view(-1)\n",
        "            labels = labels.type(torch.LongTensor)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits = model(input_features, decoder_input_ids)\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            if (i+1) % 8 == 0:\n",
        "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(train_loader)}, Train Loss: {loss.item() :.4f}')\n",
        "                train_loss = 0.0\n",
        "\n",
        "        val_loss, val_accuracy, val_f1, _ , _ = evaluate(model, val_loader, device)\n",
        "\n",
        "        if val_accuracy > best_accuracy:\n",
        "            best_accuracy = val_accuracy\n",
        "            torch.save(model.state_dict(), 'whisper_best_model.pt')\n",
        "\n",
        "        print(\"========================================================================================\")\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}, Best Accuracy: {best_accuracy:.4f}')\n",
        "        print(\"========================================================================================\")\n",
        "\n",
        "def evaluate(model, data_loader,  device):\n",
        "\n",
        "    all_labels = []\n",
        "    all_preds = []\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, batch in enumerate(data_loader):\n",
        "\n",
        "          input_features, decoder_input_ids, labels = batch\n",
        "\n",
        "          input_features = input_features.squeeze()\n",
        "          input_features = input_features.to(device)\n",
        "\n",
        "          decoder_input_ids = decoder_input_ids.squeeze()\n",
        "          decoder_input_ids = decoder_input_ids.to(device)\n",
        "\n",
        "          labels = labels.view(-1)\n",
        "          labels = labels.type(torch.LongTensor)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          logits = model(input_features, decoder_input_ids)\n",
        "\n",
        "          loss = criterion(logits, labels)\n",
        "          total_loss += loss.item()\n",
        "\n",
        "          _, preds = torch.max(logits, 1)\n",
        "          all_labels.append(labels.cpu().numpy())\n",
        "          all_preds.append(preds.cpu().numpy())\n",
        "\n",
        "    all_labels = np.concatenate(all_labels, axis=0)\n",
        "    all_preds = np.concatenate(all_preds, axis=0)\n",
        "\n",
        "    loss = total_loss / len(data_loader)\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "    return loss, accuracy, f1, all_labels, all_preds\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5VQjjTVyn6Ig",
        "outputId": "e9dd93b9-b5a8-4624-b112-ae82da612c30"
      },
      "id": "5VQjjTVyn6Ig",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SpeechClassificationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, audio_data,  text_processor):\n",
        "        self.audio_data = audio_data\n",
        "        self.text_processor = text_processor\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.audio_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "      inputs = self.text_processor(self.audio_data[index][\"audio\"][\"array\"],\n",
        "                                   return_tensors=\"pt\",\n",
        "                                   sampling_rate=self.audio_data[index][\"audio\"][\"sampling_rate\"])\n",
        "      input_features = inputs.input_features\n",
        "      decoder_input_ids = torch.tensor([[1, 1]]) * encoder.config.decoder_start_token_id\n",
        "\n",
        "      labels = np.array(self.audio_data[index]['labels'])\n",
        "\n",
        "      return input_features, decoder_input_ids, torch.tensor(labels)\n",
        "\n",
        "\n",
        "train_dataset = SpeechClassificationDataset(train_audio_dataset,  feature_extractor)\n",
        "test_dataset = SpeechClassificationDataset(test_audio_dataset,  feature_extractor)\n",
        "val_dataset = SpeechClassificationDataset(val_audio_dataset,  feature_extractor)\n",
        "\n",
        "batch_size = 8\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "-oNiAQ5fo0r9"
      },
      "id": "-oNiAQ5fo0r9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "train(model, train_loader, val_loader, optimizer, criterion, device, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CC54wmyzoYuJ",
        "outputId": "31f603e1-40c2-4174-882e-02ca5db2847a"
      },
      "id": "CC54wmyzoYuJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Batch 8/13, Train Loss: 0.6454\n",
            "========================================================================================\n",
            "Epoch 1/5, Val Loss: 0.5433, Val Accuracy: 0.8444, Val F1: 0.8394, Best Accuracy: 0.8444\n",
            "========================================================================================\n",
            "Epoch 2/5, Batch 8/13, Train Loss: 0.1197\n",
            "========================================================================================\n",
            "Epoch 2/5, Val Loss: 0.4785, Val Accuracy: 0.8222, Val F1: 0.8221, Best Accuracy: 0.8444\n",
            "========================================================================================\n",
            "Epoch 3/5, Batch 8/13, Train Loss: 0.0072\n",
            "========================================================================================\n",
            "Epoch 3/5, Val Loss: 0.4797, Val Accuracy: 0.8222, Val F1: 0.8221, Best Accuracy: 0.8444\n",
            "========================================================================================\n",
            "Epoch 4/5, Batch 8/13, Train Loss: 0.0307\n",
            "========================================================================================\n",
            "Epoch 4/5, Val Loss: 0.5107, Val Accuracy: 0.8444, Val F1: 0.8441, Best Accuracy: 0.8444\n",
            "========================================================================================\n",
            "Epoch 5/5, Batch 8/13, Train Loss: 0.0013\n",
            "========================================================================================\n",
            "Epoch 5/5, Val Loss: 0.6181, Val Accuracy: 0.8444, Val F1: 0.8441, Best Accuracy: 0.8444\n",
            "========================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluate"
      ],
      "metadata": {
        "id": "crfIt2ZOofQL"
      },
      "id": "crfIt2ZOofQL"
    },
    {
      "cell_type": "code",
      "source": [
        "#VALIDATION\n",
        "state_dict = torch.load('whisper_best_model.pt')\n",
        "\n",
        "# Create a new instance of the model and load the state dictionary\n",
        "num_labels = 2\n",
        "model = SpeechClassifier(num_labels, encoder).to(device)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "_, _, _, all_labels, all_preds = evaluate(model, val_loader, device)\n",
        "\n",
        "#VALIDATION\n",
        "print(classification_report(all_labels, all_preds))\n",
        "print(accuracy_score(all_labels, all_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0BIrK4Nogx4",
        "outputId": "69e49510-89c8-4b19-aaab-87e22539c129"
      },
      "id": "_0BIrK4Nogx4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-42f02fb44bbc>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load('whisper_best_model.pt')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.68      0.81        22\n",
            "           1       0.77      1.00      0.87        23\n",
            "\n",
            "    accuracy                           0.84        45\n",
            "   macro avg       0.88      0.84      0.84        45\n",
            "weighted avg       0.88      0.84      0.84        45\n",
            "\n",
            "0.8444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test"
      ],
      "metadata": {
        "id": "6oBBQNRTydjZ"
      },
      "id": "6oBBQNRTydjZ"
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST\n",
        "# Evaluation on test data\n",
        "_, _, _, all_labels, all_preds = evaluate(model, test_loader, device)\n",
        "\n",
        "# Generate test results\n",
        "print(classification_report(all_labels, all_preds))\n",
        "print(accuracy_score(all_labels, all_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdehLO8jyEVt",
        "outputId": "37950f37-db7b-4c45-e854-f4de9d328052"
      },
      "id": "NdehLO8jyEVt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.44      0.59        18\n",
            "           1       0.64      0.95      0.77        19\n",
            "\n",
            "    accuracy                           0.70        37\n",
            "   macro avg       0.77      0.70      0.68        37\n",
            "weighted avg       0.76      0.70      0.68        37\n",
            "\n",
            "0.7027027027027027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "More trainings using other Whisper models can be accessed through other notebooks."
      ],
      "metadata": {
        "id": "Zdmslpvq0wKg"
      },
      "id": "Zdmslpvq0wKg"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "d292282dc2964d448393ecb473c6f89c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_132629fe9e91496292f86cb40bc623a8",
              "IPY_MODEL_8bd2e279d4014af5bfa859c7cb7d70c9",
              "IPY_MODEL_4f7b7fbe3f4a48f88959a8d20a99c1e2"
            ],
            "layout": "IPY_MODEL_1944748837404dada3fd8af9ac11d3fd"
          }
        },
        "132629fe9e91496292f86cb40bc623a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e60a402ad7114725921609a879f330bd",
            "placeholder": "​",
            "style": "IPY_MODEL_e96de1b2ca67498993dceaca81f64e29",
            "value": "config.json: 100%"
          }
        },
        "8bd2e279d4014af5bfa859c7cb7d70c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e6c8edc6d5546e599f746670d89ef04",
            "max": 1983,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0b3412f6b67146bcaef301019bc1aaeb",
            "value": 1983
          }
        },
        "4f7b7fbe3f4a48f88959a8d20a99c1e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5817e5b97924be6ac45769e2723a35e",
            "placeholder": "​",
            "style": "IPY_MODEL_26c607c3043f4f13ae6fb7d6dde86224",
            "value": " 1.98k/1.98k [00:00&lt;00:00, 114kB/s]"
          }
        },
        "1944748837404dada3fd8af9ac11d3fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e60a402ad7114725921609a879f330bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e96de1b2ca67498993dceaca81f64e29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e6c8edc6d5546e599f746670d89ef04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b3412f6b67146bcaef301019bc1aaeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f5817e5b97924be6ac45769e2723a35e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26c607c3043f4f13ae6fb7d6dde86224": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53b889cc029d4412817039a59179d6d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7df6bb9be5fb4cf0bf71b101f4267e40",
              "IPY_MODEL_340af6d629a7403692c19c800586ae19",
              "IPY_MODEL_f693e41010b440b888d47a72725ca426"
            ],
            "layout": "IPY_MODEL_c1164e4fe28d4220bd383b8b6b75a36d"
          }
        },
        "7df6bb9be5fb4cf0bf71b101f4267e40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c77009d88eb349a08e257e491b98f3f7",
            "placeholder": "​",
            "style": "IPY_MODEL_692db9dae5b5403cb12c64f9df367ba7",
            "value": "model.safetensors: 100%"
          }
        },
        "340af6d629a7403692c19c800586ae19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40d581b818624a31a730b173e6b68d4f",
            "max": 290403936,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d83d89ce919843ea9cf93787aabec2ae",
            "value": 290403936
          }
        },
        "f693e41010b440b888d47a72725ca426": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a0d6a6a41ac4073a4fe1c3c758f6757",
            "placeholder": "​",
            "style": "IPY_MODEL_1130d4d9c17d4ca39e9cf851b8836158",
            "value": " 290M/290M [00:06&lt;00:00, 18.9MB/s]"
          }
        },
        "c1164e4fe28d4220bd383b8b6b75a36d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c77009d88eb349a08e257e491b98f3f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "692db9dae5b5403cb12c64f9df367ba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40d581b818624a31a730b173e6b68d4f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d83d89ce919843ea9cf93787aabec2ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4a0d6a6a41ac4073a4fe1c3c758f6757": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1130d4d9c17d4ca39e9cf851b8836158": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}